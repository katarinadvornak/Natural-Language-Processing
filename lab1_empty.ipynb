{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP 2025\n",
    "# Lab 1: Tokenization\n",
    "\n",
    "Tokenization is a fundamental step in **Natural Language Processing (NLP)** 🧠💬 that transforms raw text into structured data for computational models. In this lab, you will explore different **tokenization techniques** 📝, preprocess text data 🔍, and implement **tokenization pipelines** using popular NLP libraries 🏗️.  \n",
    "\n",
    "You will also gain **hands-on experience** with **Hugging Face Datasets 🤗📚**, while assessing the impact of tokenization choices on downstream NLP tasks. \n",
    "\n",
    "By the end of this lab, you will have a **strong foundation** in tokenization techniques and be able to apply them effectively in **real-world NLP applications** 🌍.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Learning Goals**  \n",
    "\n",
    "By the end of this lab, you should be able to:  \n",
    "\n",
    "✅ **Understand the role of tokenization in NLP** 🧠💡  \n",
    "✅ **Explain why tokenization is important** and how it affects text processing 📖🔍  \n",
    "✅ **Implement different tokenization techniques** – Apply **word** 📝, **subword** 🔢, and **character-level** 🔠 tokenization using built-in libraries.  \n",
    "✅ **Use Hugging Face Datasets** 🤗📊 – Load and preprocess text datasets efficiently.  \n",
    "✅ **Evaluate tokenization impact** 📉🔎 – Analyze how different tokenization methods influence model performance.  \n",
    "✅ **Identify challenges in tokenization** ❗🔍 – Recognize issues like **out-of-vocabulary (OOV) words**, **ambiguity**, and **multilingual tokenization** 🌍.  \n",
    "\n",
    "### Score breakdown\n",
    "\n",
    "| Exercise            | Points |\n",
    "|---------------------|--------|\n",
    "| [Exercise 1](#e1)   | 5      |\n",
    "| [Exercise 2](#e2)   | 6      |\n",
    "| [Exercise 3](#e3)   | 5      |\n",
    "| [Exercise 4](#e4)   | 12     |\n",
    "| [Exercise 5](#e5)   | 5      |\n",
    "| [Exercise 6](#e6)   | 22     |\n",
    "| [Exercise 7](#e7)   | 5      |\n",
    "| [Exercise 8](#e8)   | 5      |\n",
    "| [Exercise 9](#e9)   | 10     |\n",
    "| [Exercise 10](#e10) | 25     |\n",
    "| Total               | 100    |\n",
    "\n",
    "This score will be scaled down to 0.5 and that will be your final lab score.\n",
    "\n",
    "### 📌 **Instructions for Delivery** (📅 **Deadline: 11/Apr 18:00**, 🎭 *wildcards possible*)  \n",
    "\n",
    "✅ **Submission Requirements**  \n",
    "+ 📄 You need to submit a **PDF of your report** (use the templates provided in **LaTeX** 🖋️ (*preferred*) or **Word** 📑) and a **copy of your notebook** 📓 with the code.  \n",
    "+ ⚡ Make sure that **all cells are executed properly** ⚙️ and that **all figures/results/plots** 📊 you include in the report are also visible in your **executed notebook**.  \n",
    "\n",
    "✅ **Collaboration & Integrity**  \n",
    "+ 🗣️ While you may **discuss** the lab with others, you must **write your solutions with your group only**. If you **discuss specific tasks** with others, please **include their names** in the appendix of the report.  \n",
    "+ 📜 **Honor Code applies** to this lab. For more details, check **Syllabus §7.2** ⚖️.  \n",
    "+ 📢 **Mandatory Disclosure**:  \n",
    "   - Any **websites** 🌐 (e.g., **Stack Overflow** 💡) or **other resources** used must be **listed and disclosed**.  \n",
    "   - Any **GenAI tools** 🤖 (e.g., **ChatGPT**) used must be **explicitly mentioned**.  \n",
    "   - 🚨 **Failure to disclose these resources is a violation of academic integrity**. See **Syllabus §7.3** for details.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets~=3.2.0 in ./venv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./venv/lib/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets~=3.2.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (0.30.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from datasets~=3.2.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.13/site-packages (from aiohttp->datasets~=3.2.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->datasets~=3.2.0) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets~=3.2.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets~=3.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets~=3.2.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets~=3.2.0) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas->datasets~=3.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas->datasets~=3.2.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas->datasets~=3.2.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets~=3.2.0) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.13/site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.13/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U datasets~=3.2.0\n",
    "! pip install ipywidgets --upgrade\n",
    "! python -m pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Intro to regular expressions\n",
    "\n",
    "In this introduction section, you can practice the use of regular expressions in python. You can find the documentation here: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html). The main functions of the re module are:\n",
    "- `re.search()` - searches for a pattern in a string, returns the first match,\n",
    "- `re.findall()` - similar to `search()`, but returns a list of all matches,\n",
    "- `re.sub()` - replaces the matches with a string.\n",
    "\n",
    "All above functions accept the regular expression pattern as their argument. The patterns are strings that represent the rules for matching the text. In python they start with `r` character, e.g. `r'\\d'` is a pattern that matches a digit.\n",
    "\n",
    "Let us start with a simple example. We will search for the word \"world\" in the string \"Hello, world!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 12), match='world'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "pattern = r'world'\n",
    "match = re.search(pattern, text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `search()` function returns a match object that tells us where the match was found (`span` argument) and the exact part of the string that matched the pattern (`group` argument).\n",
    "\n",
    "Below you can find the examples from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Disjunctions\n",
    "pattern = r'[wW]oodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "pattern = r'[1234567890]' # matches any digit\n",
    "pattern = r'[0-9]' # matches any digit\n",
    "pattern = r'[A-Z]' # matches any uppercase letter\n",
    "pattern = r'[a-z]' # matches any lowercase letter\n",
    "pattern = r'[A-Za-z]' # matches any letter\n",
    "\n",
    "# Disjunctions with pipe |\n",
    "pattern = r'groundhog|Woodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "\n",
    "# Negation (only when in [])\n",
    "pattern = r'[^0-9]' # matches any character that is not a digit\n",
    "pattern = r'[^Ss]' # matches any character that is not 'S' or 's'\n",
    "pattern = r'a^b' # matches the string \"a^b\"\n",
    "\n",
    "# Quantifiers (+, *, ?, .)\n",
    "pattern = r'baa+' # matches \"ba\" followed by one or more \"a\" (e.g. \"baa\", \"baaa\", \"baaaa\", ...)\n",
    "pattern = r'oo*h' # matches \"o\" followed by zero or more \"o\" and then \"h\" (e.g. \"oh\", \"ooh\", \"oooh\", ...)\n",
    "pattern = r'colou?r' # matches \"color\" and \"colour\"\n",
    "pattern = r'beg.n' # matches \"begun\", \"begin\", \"begnn\", ...\n",
    "\n",
    "# Anchors (^, $)\n",
    "pattern = r'^Hello' # matches \"Hello\" at the beginning of the string\n",
    "pattern = r'world!$' # matches \"world!\" at the end of the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Huggingface datasets\n",
    "\n",
    "For this lab, we will use the **Hugging Face Datasets** library ([Hugging Face Datasets](https://huggingface.co/datasets)), which provides an extensive collection of ready-to-use NLP datasets. The library is designed to be lightweight, efficient, and compatible with popular deep learning frameworks such as PyTorch and TensorFlow.  \n",
    "\n",
    "You can find the full documentation and tutorials here:  \n",
    "📌 [Hugging Face Datasets Documentation](https://huggingface.co/docs/datasets/en/index)  \n",
    "\n",
    "### **Why use Hugging Face Datasets?**  \n",
    "- **Easy Access:** Load datasets with a single command without manual downloads.  \n",
    "- **Standardized Format:** Datasets come in a unified structure, making them easy to preprocess and integrate into ML pipelines.  \n",
    "- **Large Collection:** Provides datasets for a wide range of NLP tasks, including classification, translation, summarization, and more.  \n",
    "- **Seamless Integration:** Works with `transformers` and `sklearn` for preprocessing and model training.  \n",
    "\n",
    "### **Dataset for this lab: TweetEval - Emoji Subset**  \n",
    "\n",
    "In this lab, we will work with the **TweetEval** dataset, specifically the **emoji** subset. The TweetEval dataset is a benchmark for evaluating NLP models on Twitter-related tasks, covering tasks such as sentiment analysis, hate speech detection, and irony detection.  \n",
    "\n",
    "For tokenization, we will focus only on the **text** (the content of the tweets), but we will also examine the **labels** to understand the dataset structure.  \n",
    "\n",
    "🔗 The dataset description and details are available in its dataset card: [**TweetEval Dataset**](https://huggingface.co/datasets/cardiffnlp/tweet_eval) \n",
    "\n",
    "💡 Exploring More Datasets\n",
    "Hugging Face provides a vast selection of datasets across different NLP tasks. You can browse and explore more at:\n",
    "🔗 [Hugging Face Datasets Collection](https://huggingface.co/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tweet_ds = datasets.load_dataset('tweet_eval', 'emoji')\n",
    "print(tweet_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The loaded dataset contains three subsets (“train”, “validation”, and “test”). Each consists of two columns: “text” and “label”. Label is an integer from 0 to 19 representing an emoji. See the dataset's card for more information. We can access the elements of the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Sunday afternoon walking through Venice in the sun with @user ️ ️ ️ @ Abbot Kinney, Venice', 'label': 12}\n",
      "{'text': \"Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\", 'label': 19}\n",
      "{'text': 'Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…', 'label': 0}\n",
      "{'text': '️ ️ ️ ️ @ Toys\"R\"Us', 'label': 0}\n",
      "{'text': 'Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme', 'label': 2}\n",
      "{'text': '#sandiego @ San Diego, California', 'label': 11}\n",
      "{'text': 'My little ️ ️ ️ ️ ️ #ObsessedWithMyDog @ Cafe Solstice Capitol Hill', 'label': 0}\n",
      "{'text': 'More #tinyepic things #tinyepicwestern, this one is crazy @user I may be one of your…', 'label': 19}\n",
      "{'text': 'Last night ️ @ Omnia Night Club At Caesars Palace', 'label': 0}\n",
      "{'text': 'friendship at its finest. ....#pixar #toystory #buzz #woody #friends #friendship #bff…', 'label': 7}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tweet_ds['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can easily cast the dataset to the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "0      Sunday afternoon walking through Venice in the...     12\n",
      "1      Time for some BBQ and whiskey libations. Chomp...     19\n",
      "2      Love love love all these people ️ ️ ️ #friends...      0\n",
      "3                                    ️ ️ ️ ️ @ Toys\"R\"Us      0\n",
      "4      Man these are the funniest kids ever!! That fa...      2\n",
      "...                                                  ...    ...\n",
      "44995  Here to celebrate the Nunez wedding! Love my b...      0\n",
      "44996  1 night in Paris.... Wait... @ Paris Las Vegas...      1\n",
      "44997  Be safe this weekend everyone. #happylaborday ...     11\n",
      "44998          Pizza (@ Five50 - @user in Las Vegas, NV)      1\n",
      "44999  my mini is perfect, no one deserves her @ Las ...     13\n",
      "\n",
      "[45000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tweet_train_df = pd.DataFrame(tweet_ds['train'])\n",
    "print(tweet_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can plot the distribution of the labels in the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAG0CAYAAAAsOB08AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMVNJREFUeJzt3XuczeXe//H3WnPGzGAyp23MTFHMI3Jsmg5SzTZJB+WurQhFqj2SlKIDlUI6iHKztSvuUtG9O7JTImzHNBVCKGRKMyqZCZlh5nP/0W/Wz8qhZtaMmWu8no/H9/Gwvte1rs/1nWWteX8P6zseMzMBAAA4xFvdEwAAACgvAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHOCq3sCVaW0tFQ7duxQZGSkPB5PdU8HAAD8CWamX375RYmJifJ6j36cpdYGmB07digpKam6pwEAACogNzdXjRs3Pmp7rQ0wkZGRkn77AURFRVXzbAAAwJ9RWFiopKQk3+/xo6m1AabstFFUVBQBBgAAx/zR5R9cxAsAAJxDgAEAAM4hwAAAAOfU2mtgAAA4nkpKSnTgwIHqnkaNFxISoqCgoIDHIcAAABAAM1NeXp52795d3VNxRv369RUfHx/QfdoIMAAABKAsvMTGxqpOnTrcPPUYzEz79u3Tzp07JUkJCQkVHosAAwBABZWUlPjCS0xMTHVPxwkRERGSpJ07dyo2NrbCp5O4iBcAgAoqu+alTp061TwTt5T9vAK5ZogAAwBAgDhtVD6V8fMiwAAAAOcQYAAAgHO4iBcAgCqQMmzOca23bWzX41qvunEEBgCAE1CnTp00ePDgGj/m0RBgAACAcwgwAACcYPr27atFixZpwoQJ8ng88ng82rZtm7744gt16dJF9erVU1xcnK6//nr9+OOPkqSFCxcqNDRU//nPf3zjjBs3TrGxscrPzz/qmFXlhLwGprznJU+084oAgNptwoQJ2rRpk04//XQ9/PDDkn77G0Vnnnmm+vfvr/Hjx+vXX3/VPffco2uuuUYLFizwnR66/vrrtXr1am3ZskUPPPCAXn/9dcXFxR1xzEaNGlXZNpyQAQYAgBNZdHS0QkNDVadOHcXHx0uSHnnkEbVp00ajR4/29XvhhReUlJSkTZs26dRTT9UjjzyiefPmacCAAfriiy/Up08fXX755UcdsyoRYAAAgFavXq2PPvpI9erVO6zt66+/1qmnnqrQ0FDNmDFDrVq1UnJyssaPH18NM/0NAQYAAGjPnj267LLL9Nhjjx3WdugfXVy2bJkkadeuXdq1a5fq1q173OZ4KAIMAAAnoNDQUJWUlPget23bVv/617+UkpKi4OAjx4Ovv/5ad9xxh5577jnNnDlTffr00Ycffiiv13vEMasS30ICAOAElJKSopUrV2rbtm368ccflZ2drV27dunaa6/VqlWr9PXXX+v999/XDTfcoJKSEpWUlKhXr17KysrSDTfcoBdffFFr1qzRk08+edQxS0tLq2z+HIEBAKAK1PRvsN51113q06eP0tLS9Ouvv2rr1q1aunSp7rnnHnXu3FlFRUVKTk7WxRdfLK/Xq1GjRumbb77R7NmzJf12Wmnq1Km69tpr1blzZ51xxhlHHDMlJaVK5u8xM6uSkatZYWGhoqOjVVBQoKioKL82vkYNAKgM+/fv19atW5Wamqrw8PDqno4zjvVzO9bv70NxCgkAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAACFBVfl24NqqMnxdfowYAoIJCQ0Pl9Xq1Y8cONWrUSKGhofJ4PNU9rRrLzFRcXKwffvhBXq9XoaGhFR6LAAMAQAV5vV6lpqbq+++/144dO6p7Os6oU6eOmjRp4ruDb0UQYAAACEBoaKiaNGmigwcPHrfb6LssKChIwcHBAR+pIsAAABAgj8ejkJAQhYSEVPdUThhcxAsAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxTrgBTUlKiBx54QKmpqYqIiNApp5yiUaNGycx8fcxMI0aMUEJCgiIiIpSZmanNmzf7jbNr1y717NlTUVFRql+/vvr166c9e/b49VmzZo3OO+88hYeHKykpSePGjQtgMwEAQG1SrgDz2GOPafLkyXr22We1YcMGPfbYYxo3bpyeeeYZX59x48Zp4sSJmjJlilauXKm6desqKytL+/fv9/Xp2bOn1q1bp3nz5mn27NlavHixBgwY4GsvLCxU586dlZycrJycHD3++ON68MEHNXXq1ErYZAAA4DqPHXr45A9ceumliouL0/PPP+9b1717d0VEROjll1+WmSkxMVF33nmn7rrrLklSQUGB4uLiNG3aNPXo0UMbNmxQWlqaVq1apfbt20uS5s6dq0suuUTffvutEhMTNXnyZN13333Ky8tTaGioJGnYsGF666239OWXX/6puRYWFio6OloFBQWKiorya0sZNufPbrIkadvYruXqDwAAKuZYv78PVa4jMGeffbbmz5+vTZs2SZJWr16tJUuWqEuXLpKkrVu3Ki8vT5mZmb7nREdHKz09XcuXL5ckLV++XPXr1/eFF0nKzMyU1+vVypUrfX06duzoCy+SlJWVpY0bN+rnn38+4tyKiopUWFjotwAAgNopuDydhw0bpsLCQjVv3lxBQUEqKSnRo48+qp49e0qS8vLyJElxcXF+z4uLi/O15eXlKTY21n8SwcFq2LChX5/U1NTDxihra9CgwWFzGzNmjB566KHybA4AAHBUuY7AzJo1SzNmzNArr7yiTz/9VNOnT9cTTzyh6dOnV9X8/rThw4eroKDAt+Tm5lb3lAAAQBUp1xGYoUOHatiwYerRo4ckqWXLlvrmm280ZswY9enTR/Hx8ZKk/Px8JSQk+J6Xn5+v1q1bS5Li4+O1c+dOv3EPHjyoXbt2+Z4fHx+v/Px8vz5lj8v6/F5YWJjCwsLKszkAAMBR5ToCs2/fPnm9/k8JCgpSaWmpJCk1NVXx8fGaP3++r72wsFArV65URkaGJCkjI0O7d+9WTk6Or8+CBQtUWlqq9PR0X5/FixfrwIEDvj7z5s3TaaeddsTTRwAA4MRSrgBz2WWX6dFHH9WcOXO0bds2vfnmm3rqqad05ZVXSpI8Ho8GDx6sRx55RO+8847Wrl2r3r17KzExUd26dZMktWjRQhdffLFuuukmffzxx1q6dKkGDhyoHj16KDExUZJ03XXXKTQ0VP369dO6des0c+ZMTZgwQUOGDKncrQcAAE4q1ymkZ555Rg888ID+/ve/a+fOnUpMTNTNN9+sESNG+Prcfffd2rt3rwYMGKDdu3fr3HPP1dy5cxUeHu7rM2PGDA0cOFAXXXSRvF6vunfvrokTJ/rao6Oj9cEHHyg7O1vt2rXTSSedpBEjRvjdKwYAAJy4ynUfGJdwHxgAANxTJfeBAQAAqAkIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA55Q4w3333nXr16qWYmBhFRESoZcuW+uSTT3ztZqYRI0YoISFBERERyszM1ObNm/3G2LVrl3r27KmoqCjVr19f/fr10549e/z6rFmzRuedd57Cw8OVlJSkcePGVXATAQBAbVOuAPPzzz/rnHPOUUhIiN577z2tX79eTz75pBo0aODrM27cOE2cOFFTpkzRypUrVbduXWVlZWn//v2+Pj179tS6des0b948zZ49W4sXL9aAAQN87YWFhercubOSk5OVk5Ojxx9/XA8++KCmTp1aCZsMAABc5zEz+7Odhw0bpqVLl+o///nPEdvNTImJibrzzjt11113SZIKCgoUFxenadOmqUePHtqwYYPS0tK0atUqtW/fXpI0d+5cXXLJJfr222+VmJioyZMn67777lNeXp5CQ0N9td966y19+eWXf2quhYWFio6OVkFBgaKiovzaUobN+bObLEnaNrZrufoDAICKOdbv70OV6wjMO++8o/bt2+vqq69WbGys2rRpo+eee87XvnXrVuXl5SkzM9O3Ljo6Wunp6Vq+fLkkafny5apfv74vvEhSZmamvF6vVq5c6evTsWNHX3iRpKysLG3cuFE///zzEedWVFSkwsJCvwUAANRO5QowW7Zs0eTJk9WsWTO9//77uvXWWzVo0CBNnz5dkpSXlydJiouL83teXFycry0vL0+xsbF+7cHBwWrYsKFfnyONcWiN3xszZoyio6N9S1JSUnk2DQAAOKRcAaa0tFRt27bV6NGj1aZNGw0YMEA33XSTpkyZUlXz+9OGDx+ugoIC35Kbm1vdUwIAAFWkXAEmISFBaWlpfutatGih7du3S5Li4+MlSfn5+X598vPzfW3x8fHauXOnX/vBgwe1a9cuvz5HGuPQGr8XFhamqKgovwUAANRO5Qow55xzjjZu3Oi3btOmTUpOTpYkpaamKj4+XvPnz/e1FxYWauXKlcrIyJAkZWRkaPfu3crJyfH1WbBggUpLS5Wenu7rs3jxYh04cMDXZ968eTrttNP8vvEEAABOTOUKMHfccYdWrFih0aNH66uvvtIrr7yiqVOnKjs7W5Lk8Xg0ePBgPfLII3rnnXe0du1a9e7dW4mJierWrZuk347YXHzxxbrpppv08ccfa+nSpRo4cKB69OihxMRESdJ1112n0NBQ9evXT+vWrdPMmTM1YcIEDRkypHK3HgAAOCm4PJ07dOigN998U8OHD9fDDz+s1NRUPf300+rZs6evz9133629e/dqwIAB2r17t84991zNnTtX4eHhvj4zZszQwIEDddFFF8nr9ap79+6aOHGirz06OloffPCBsrOz1a5dO5100kkaMWKE371iAADAiatc94FxCfeBAQDAPVVyHxgAAICagAADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4Jru4J1FYpw+aUq/+2sV2raCYAANQ+HIEBAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOX6N2GF/VBgCcqDgCAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4JyAAszYsWPl8Xg0ePBg37r9+/crOztbMTExqlevnrp37678/Hy/523fvl1du3ZVnTp1FBsbq6FDh+rgwYN+fRYuXKi2bdsqLCxMTZs21bRp0wKZKgAAqEUqHGBWrVqlf/zjH2rVqpXf+jvuuEPvvvuuXn/9dS1atEg7duzQVVdd5WsvKSlR165dVVxcrGXLlmn69OmaNm2aRowY4euzdetWde3aVRdccIE+//xzDR48WP3799f7779f0ekCAIBapEIBZs+ePerZs6eee+45NWjQwLe+oKBAzz//vJ566ildeOGFateunV588UUtW7ZMK1askCR98MEHWr9+vV5++WW1bt1aXbp00ahRozRp0iQVFxdLkqZMmaLU1FQ9+eSTatGihQYOHKj/+q//0vjx4ythkwEAgOsqFGCys7PVtWtXZWZm+q3PycnRgQMH/NY3b95cTZo00fLlyyVJy5cvV8uWLRUXF+frk5WVpcLCQq1bt87X5/djZ2Vl+cY4kqKiIhUWFvotAACgdgou7xNee+01ffrpp1q1atVhbXl5eQoNDVX9+vX91sfFxSkvL8/X59DwUtZe1nasPoWFhfr1118VERFxWO0xY8booYceKu/mAAAAB5XrCExubq5uv/12zZgxQ+Hh4VU1pwoZPny4CgoKfEtubm51TwkAAFSRcgWYnJwc7dy5U23btlVwcLCCg4O1aNEiTZw4UcHBwYqLi1NxcbF2797t97z8/HzFx8dLkuLj4w/7VlLZ4z/qExUVdcSjL5IUFhamqKgovwUAANRO5QowF110kdauXavPP//ct7Rv3149e/b0/TskJETz58/3PWfjxo3avn27MjIyJEkZGRlau3atdu7c6eszb948RUVFKS0tzdfn0DHK+pSNAQAATmzlugYmMjJSp59+ut+6unXrKiYmxre+X79+GjJkiBo2bKioqCjddtttysjI0FlnnSVJ6ty5s9LS0nT99ddr3LhxysvL0/3336/s7GyFhYVJkm655RY9++yzuvvuu3XjjTdqwYIFmjVrlubMmVMZ2wwAABxX7ot4/8j48ePl9XrVvXt3FRUVKSsrS//93//taw8KCtLs2bN16623KiMjQ3Xr1lWfPn308MMP+/qkpqZqzpw5uuOOOzRhwgQ1btxY//znP5WVlVXZ0wUAAA4KOMAsXLjQ73F4eLgmTZqkSZMmHfU5ycnJ+ve//33McTt16qTPPvss0OkBAIBaiL+FBAAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4J7i6JwDgz0kZNqdc/beN7VpFMwGA6scRGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAc7gPDKpVee9tIpX//ibcPwUAah+OwAAAAOeUK8CMGTNGHTp0UGRkpGJjY9WtWzdt3LjRr8/+/fuVnZ2tmJgY1atXT927d1d+fr5fn+3bt6tr166qU6eOYmNjNXToUB08eNCvz8KFC9W2bVuFhYWpadOmmjZtWsW2EAAA1DrlOoW0aNEiZWdnq0OHDjp48KDuvfdede7cWevXr1fdunUlSXfccYfmzJmj119/XdHR0Ro4cKCuuuoqLV26VJJUUlKirl27Kj4+XsuWLdP333+v3r17KyQkRKNHj5Ykbd26VV27dtUtt9yiGTNmaP78+erfv78SEhKUlZVVyT8CIHCcpgKA46tcAWbu3Ll+j6dNm6bY2Fjl5OSoY8eOKigo0PPPP69XXnlFF154oSTpxRdfVIsWLbRixQqdddZZ+uCDD7R+/Xp9+OGHiouLU+vWrTVq1Cjdc889evDBBxUaGqopU6YoNTVVTz75pCSpRYsWWrJkicaPH3/UAFNUVKSioiLf48LCwnL9IHC443F9CgAAFRHQNTAFBQWSpIYNG0qScnJydODAAWVmZvr6NG/eXE2aNNHy5cslScuXL1fLli0VFxfn65OVlaXCwkKtW7fO1+fQMcr6lI1xJGPGjFF0dLRvSUpKCmTTAABADVbhbyGVlpZq8ODBOuecc3T66adLkvLy8hQaGqr69ev79Y2Li1NeXp6vz6Hhpay9rO1YfQoLC/Xrr78qIiLisPkMHz5cQ4YM8T0uLCwkxAAnIE7nASeGCgeY7OxsffHFF1qyZEllzqfCwsLCFBYWVt3TAAAAx0GFTiENHDhQs2fP1kcffaTGjRv71sfHx6u4uFi7d+/265+fn6/4+Hhfn99/K6ns8R/1iYqKOuLRFwAAcGIpV4AxMw0cOFBvvvmmFixYoNTUVL/2du3aKSQkRPPnz/et27hxo7Zv366MjAxJUkZGhtauXaudO3f6+sybN09RUVFKS0vz9Tl0jLI+ZWMAAIATW7lOIWVnZ+uVV17R22+/rcjISN81K9HR0YqIiFB0dLT69eunIUOGqGHDhoqKitJtt92mjIwMnXXWWZKkzp07Ky0tTddff73GjRunvLw83X///crOzvadArrlllv07LPP6u6779aNN96oBQsWaNasWZozp/zfigEAALVPuY7ATJ48WQUFBerUqZMSEhJ8y8yZM319xo8fr0svvVTdu3dXx44dFR8frzfeeMPXHhQUpNmzZysoKEgZGRnq1auXevfurYcfftjXJzU1VXPmzNG8efN0xhln6Mknn9Q///lP7gEDAAAklfMIjJn9YZ/w8HBNmjRJkyZNOmqf5ORk/fvf/z7mOJ06ddJnn31WnukBAIATBH8LCQAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOKdffQgIAAO5JGTan3M/ZNrZrFcyk8nAEBgAAOIcAAwAAnEOAAQAAziHAAAAA53ARL4DjpjZeSAhUhvK+N3hfEGAAADgmwkXNxCkkAADgHAIMAABwDqeQAEji+hQAbiHAAEANxHUXwLERYAAAVYKjeqhKBBgAABCw4x1YuYgXAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAc7gMDACcgbjIH13EEBgAAOIcjMABQTvydopqD1+LExREYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOAcAgwAAHAOAQYAADiHAAMAAJxDgAEAAM4hwAAAAOcQYAAAgHMIMAAAwDkEGAAA4BwCDAAAcA4BBgAAOIcAAwAAnEOAAQAAziHAAAAA5xBgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzanSAmTRpklJSUhQeHq709HR9/PHH1T0lAABQA9TYADNz5kwNGTJEI0eO1KeffqozzjhDWVlZ2rlzZ3VPDQAAVLMaG2Ceeuop3XTTTbrhhhuUlpamKVOmqE6dOnrhhReqe2oAAKCaBVf3BI6kuLhYOTk5Gj58uG+d1+tVZmamli9ffsTnFBUVqaioyPe4oKBAklRYWHhY39KifeWaz5HG+CO1oUZ5x68tNWria3E8atTE1+J41KiJr8XxqFETX4vjUaMmvhbHo0ZNfC2OVqNsnZkd+8lWA3333XcmyZYtW+a3fujQoXbmmWce8TkjR440SSwsLCwsLCy1YMnNzT1mVqiRR2AqYvjw4RoyZIjvcWlpqXbt2qWYmBh5PJ4/fH5hYaGSkpKUm5urqKioKpkjNWrG+NSoWTVqwzZQo+aMT42aVaMi45uZfvnlFyUmJh6zX40MMCeddJKCgoKUn5/vtz4/P1/x8fFHfE5YWJjCwsL81tWvX7/ctaOioqrsPwo1atb41KhZNWrDNlCj5oxPjZpVo7zjR0dH/2GfGnkRb2hoqNq1a6f58+f71pWWlmr+/PnKyMioxpkBAICaoEYegZGkIUOGqE+fPmrfvr3OPPNMPf3009q7d69uuOGG6p4aAACoZjU2wPztb3/TDz/8oBEjRigvL0+tW7fW3LlzFRcXVyX1wsLCNHLkyMNOQ1Hj+NeoDdtAjZozPjVqVo3asA3UqBnje8z+6HtKAAAANUuNvAYGAADgWAgwAADAOQQYAADgHAIMAABwDgEGNRLXlgMAjqXGfo26qv3444964YUXtHz5cuXl5UmS4uPjdfbZZ6tv375q1KhRNc/wxBYWFqbVq1erRYsW1T0VVLLvv/9ekydP1pIlS/T999/L6/Xq5JNPVrdu3dS3b18FBQVV9xQBOOCE/Br1qlWrlJWVpTp16igzM9N3b5n8/HzNnz9f+/bt0/vvv6/27dtX6Txyc3M1cuRIvfDCCxUe49dff1VOTo4aNmyotLQ0v7b9+/dr1qxZ6t27d0Dz3LBhg1asWKGMjAw1b95cX375pSZMmKCioiL16tVLF154YYXHPvTvVx1qwoQJ6tWrl2JiYiRJTz31VIVr/N7evXs1a9YsffXVV0pISNC1117rq1NRn376qRo0aKDU1FRJ0ksvvaQpU6Zo+/btSk5O1sCBA9WjR4+Aatx222265pprdN555wU0zh959tln9fHHH+uSSy5Rjx499NJLL2nMmDEqLS3VVVddpYcffljBwRXb9/nkk0+UmZmppk2bKiIiQsuXL9d1112n4uJivf/++0pLS9PcuXMVGRlZyVsFoNaphD8e7Zz09HQbMGCAlZaWHtZWWlpqAwYMsLPOOqvK5/H555+b1+ut8PM3btxoycnJ5vF4zOv1WseOHW3Hjh2+9ry8vIDGNzN77733LDQ01Bo2bGjh4eH23nvvWaNGjSwzM9MuvPBCCwoKsvnz51d4fI/HY61bt7ZOnTr5LR6Pxzp06GCdOnWyCy64IKBtaNGihf30009mZrZ9+3ZLSUmx6Oho69ChgzVs2NBiY2Nty5YtAdVo1aqVzZs3z8zMnnvuOYuIiLBBgwbZ5MmTbfDgwVavXj17/vnnA6pR9jo3a9bMxo4da99//31A4x3JqFGjLDIy0rp3727x8fE2duxYi4mJsUceecRGjx5tjRo1shEjRlR4/HPOOccefPBB3+OXXnrJ0tPTzcxs165d1rp1axs0aFDA22FmVlRUZDNnzrTBgwdbjx49rEePHjZ48GCbNWuWFRUVVUqNY8nLy7OHHnoo4HFyc3Ptl19+OWx9cXGxLVq0KODxf/zxR1uwYIHvPfLDDz/Y2LFj7aGHHrL169cHPP7RpKam2qZNm6pk7NLSUluwYIFNnTrV3n33XSsuLg54zNzcXPvhhx98jxcvXmzXXXednXvuudazZ09btmxZQOM/8cQTtm3btkCn+Yfeffdde+CBB2zJkiVmZjZ//nzr0qWLZWVl2T/+8Y9KqbFv3z57/vnn7YYbbrCLL77YLrnkEhs4cKB9+OGHlTJ+mRMywISHh9uGDRuO2r5hwwYLDw8PuM7bb799zGX8+PEBBYxu3bpZ165d7YcffrDNmzdb165dLTU11b755hszq5wAk5GRYffdd5+Zmb366qvWoEEDu/fee33tw4YNs7/+9a8VHn/MmDGWmpp6WAgKDg62devWVXjcQ3k8HsvPzzczs549e9rZZ59tu3fvNjOzX375xTIzM+3aa68NqEZERITvw6dNmzY2depUv/YZM2ZYWlpaQDU8Ho99+OGHdvvtt9tJJ51kISEhdvnll9u7775rJSUlAY1d5pRTTrF//etfZvZbwA4KCrKXX37Z1/7GG29Y06ZNKzx+RESEff31177HJSUlFhISYnl5eWZm9sEHH1hiYmKFxy+zefNmO/nkky08PNzOP/98u+aaa+yaa66x888/38LDw61p06a2efPmgOscS6A7KDt27LAOHTqY1+u1oKAgu/766/2CTGW8v1euXGnR0dHm8XisQYMG9sknn1hqaqo1a9bMTjnlFIuIiLCcnJyAakyYMOGIS1BQkA0fPtz3OBBdunTxvad/+uknS09PN4/HY40aNTKv12vNmze3nTt3BlTjzDPPtHfffdfMzN566y3zer12+eWX2z333GNXXnmlhYSE+NorwuPxWFBQkGVmZtprr71WJSF7ypQpFhwcbO3atbOoqCh76aWXLDIy0vr3728333yzRURE2NNPPx1Qjc2bN1tycrLFxsZaUlKSeTwe69q1q6Wnp1tQUJBdffXVduDAgUrZnhMywKSkpNj06dOP2j59+nRLTk4OuE7ZHrPH4znqEsgHUGxsrK1Zs8b3uLS01G655RZr0qSJff3115XyARcVFeX7oC8pKbHg4GD79NNPfe1r1661uLi4gGp8/PHHduqpp9qdd97p21OqqgBz8skn2wcffODXvnTpUktKSgqoRkxMjH3yySdm9tvr8vnnn/u1f/XVVxYRERFQjUO3o7i42GbOnGlZWVkWFBRkiYmJdu+99wb8SzkiIsIXgM3MQkJC7IsvvvA93rZtm9WpU6fC4ycnJ/v2/Mx++yXt8Xhs3759Zma2devWStl5yMzMtCuuuMIKCgoOaysoKLArrrjCOnfuHFCN1atXH3OZOXNmQO+/3r17W3p6uq1atcrmzZtn7dq1s/bt29uuXbvM7LcA4/F4AtqGzMxM69+/vxUWFtrjjz9ujRs3tv79+/vab7jhBuvWrVtANTwejzVu3NhSUlL8Fo/HY3/5y18sJSXFUlNTA65R9t649dZbLS0tzXdUNTc319q1a2e33HJLQDXq1q3rGzM9Pd3Gjh3r1/7MM89YmzZtKjy+x+OxF1980a644goLCQmxmJgYu/32223t2rUBzftQaWlpvp2rBQsWWHh4uE2aNMnX/uKLL1qLFi0CqtGlSxe7+eabfWc4xo4da126dDEzs02bNllKSoqNHDkyoBplTsgA8+yzz1pYWJgNGjTI3n77bVuxYoWtWLHC3n77bRs0aJBFRET4vagVlZiYaG+99dZR2z/77LOAPuAiIyOPeIg3OzvbGjdubIsXL66UAPPVV1/5HterV89vD3rbtm2V8gvnl19+sd69e1urVq1s7dq1FhISUqkBpmzvKzEx8bAPhMrYhl69elm/fv3MzOzqq6+2+++/36999OjR1rJly4BqHPohfahvvvnGRo4cacnJyQG/3qmpqfbee++Z2W8fNl6v12bNmuVrnzNnjqWkpFR4/Ntvv91OP/10e++992zBggV2wQUXWKdOnXztc+fOtVNOOaXiG/D/REREHPODf82aNZUSKI+2g1K2PpDXIzEx0VauXOl7vH//frvsssusdevW9tNPP1XKDkqDBg18nyHFxcXm9Xr9aubk5Nhf/vKXgGrcfPPN1rp168M+q6pqJ+W0006zt99+26/9ww8/DDgkRUdH2+rVq83st52Usn+X+eqrrwIK94duQ35+vj322GPWvHlz83q91qFDB5s6daoVFhZWfAPsyDsoh75Ptm7dGtA2mJnVqVPH79RgUVGRhYSE2I8//mhmvx29CuQz5FAnZIAxM3vttdcsPT3dgoODfR86wcHBlp6ebjNnzqyUGpdddpk98MADR23//PPPA9qD6tChg/3P//zPEduys7Otfv36AX/AtWrVyvcLzey3Iy6HHv5bvHhxwB8Mh3r11VctLi7OvF5vpX64tWzZ0tq0aWP16tWz//3f//VrX7RoUcAf0t99952lpKRYx44dbciQIRYREWHnnnuu3XTTTdaxY0cLDQ21OXPmBFTjaAGmTGlp6WFHl8rr/vvvt0aNGln//v0tNTXVhg0bZk2aNLHJkyfblClTLCkpye64444Kj//LL7/YNddc43vfnX322X7XH73//vt+gamiEhISjnk4/5133rGEhISAasTExNjzzz9v27ZtO+IyZ86cgN5/devWPewakQMHDli3bt2sVatWtmbNmoDf33Xr1rWtW7f6Hv9+B+Wbb76plB2UN954w5KSkuyZZ57xravsAFO2kxIbG+t31NDst52UsLCwgGpcfvnlNmzYMDMzy8rKOuy013PPPWfNmjWr8PhHe38vXrzY+vTpY3Xr1rW6detWeHwz8+3Ymv32meXxePw+lxYuXGiNGzcOqEZiYqLfaceff/7ZPB6PL3xt2bIl4NeizAkbYMoUFxfbjh07bMeOHZVyodehFi9e7PfL//f27NljCxcurPD4o0eP9h2aO5Jbb7014EPMkydPttmzZx+1ffjw4b4jD5UlNzfX3nrrLduzZ0+ljPfggw/6LXPnzvVrv+uuu6xHjx4B1/n555/tnnvusbS0NAsPD7fQ0FBLTk626667zlatWhXw+CkpKb69mKpSUlJijz76qF166aU2evRoKy0ttVdffdWSkpIsJibG+vbtWymvy6+//nrEC1MrywMPPGANGjSwp556ylavXm15eXmWl5dnq1evtqeeesoaNmwY8GHszp0726hRo47aHugOSsuWLQ8L22b/P8Q0adIk4ADTvHlzv+vPZs+e7TudZ2a2YsWKgH+hlfn222/twgsvtIsvvti+//77Sg8wl1xyiV155ZXWoEGDw8LrihUrAj7VvX79eouJibHevXvbqFGjrF69etarVy979NFHrXfv3hYWFmYvvvhihcf3er3H3EEpKCg47Nq68srOzrZmzZrZI488Ymeeeab16dPHmjdvbu+9957NnTvXWrZsaTfeeGNANfr06WPnn3++bdiwwbZs2WJ/+9vf/E6tLVy4MOBT9mVO+AADoHYaO3asJSQk+E7llJ3WSUhIsMceeyzg8d944w176aWXjtq+a9cumzZtWoXHv/vuu496nc6BAwfs8ssvD3gH5cEHH7RXX331qO333nuvXXXVVQHVOFRpaamNHj3a4uPjLSgoqNICTN++ff2W3x9FHzp0qGVlZQVc56uvvrIePXpYZGSk78h9SEiInX322fbmm28GNPYfHWGtDHv27LGbbrrJTj/9dBswYIAVFRXZ448/bqGhoebxeKxTp04BzyE/P9/OOuss3/suOTnZ77rJ119/3SZOnBjoppiZ2Ql5HxgAJ46tW7f63ayy7F49Nd3Bgwe1b98+RUVFHbX9u+++U3JycpXNYd++fQoKClJYWFiljpuTk6MlS5aod+/eatCgQaWOfSR79+5VUFCQwsPDK2U8M9POnTtVWlqqk046SSEhIZUybnXZv3+/Dhw4UKn3X9q8ebOKiorUvHnzCt836o/wpwQA1GqpqanKyMhQRkaGL7zk5ubqxhtvrNK6gdYIDg4+aniRfruj8UMPPVTh8f+Mn376Sbfeemulj9uuXTvdfvvtatCgwXF5LXbt2qW///3vlTaex+NRXFycEhISfOGlqrejKscPDw9XZGRkpdZo1qyZTj/99MPCS2XW4AgMgBPO6tWr1bZtW5WUlDhbozZsAzVqzvgu1jhh/xYSgNrrnXfeOWb7li1banyN2rAN1Kg549emGmU4AgOg1vF6vfJ4PMf8q+YejyegvcCqrlEbtoEaNWf82lTDVyvgEQCghklISNAbb7yh0tLSIy6ffvppja9RG7aBGjVn/NpUowwBBkCt065dO+Xk5By1/Y/2EGtCjdqwDdSoOePXphpluAYGQK0zdOhQ7d2796jtTZs21UcffVSja9SGbaBGzRm/NtUowzUwAADAOZxCAgAAziHAAAAA5xBgAACAcwgwAADAOQQYANWiU6dOGjx48J/qu3DhQnk8Hu3evTugmikpKXr66acDGgNAzUCAAQAAziHAAAAA5xBgAFS7l156Se3bt1dkZKTi4+N13XXXaefOnYf1W7p0qVq1aqXw8HCdddZZ+uKLL/zalyxZovPOO08RERFKSkrSoEGDjnlTLQDuIsAAqHYHDhzQqFGjtHr1ar311lvatm2b+vbte1i/oUOH6sknn9SqVavUqFEjXXbZZTpw4IAk6euvv9bFF1+s7t27a82aNZo5c6aWLFmigQMHHuetAXA88KcEAFS7G2+80ffvk08+WRMnTlSHDh20Z88e1atXz9c2cuRI/fWvf5UkTZ8+XY0bN9abb76pa665RmPGjFHPnj19FwY3a9ZMEydO1Pnnn6/JkycrPDz8uG4TgKrFERgA1S4nJ0eXXXaZmjRposjISJ1//vmSpO3bt/v1y8jI8P27YcOGOu2007RhwwZJ0urVqzVt2jTVq1fPt2RlZam0tFRbt249fhsD4LjgCAyAarV3715lZWUpKytLM2bMUKNGjbR9+3ZlZWWpuLj4T4+zZ88e3XzzzRo0aNBhbU2aNKnMKQOoAQgwAKrVl19+qZ9++kljx45VUlKSJOmTTz45Yt8VK1b4wsjPP/+sTZs2qUWLFpKktm3bav369WratOnxmTiAasUpJADVqkmTJgoNDdUzzzyjLVu26J133tGoUaOO2Pfhhx/W/Pnz9cUXX6hv37466aST1K1bN0nSPffco2XLlmngwIH6/PPPtXnzZr399ttcxAvUUgQYANWqUaNGmjZtml5//XWlpaVp7NixeuKJJ47Yd+zYsbr99tvVrl075eXl6d1331VoaKgkqVWrVlq0aJE2bdqk8847T23atNGIESOUmJh4PDcHwHHiMTOr7kkAAACUB0dgAACAcwgwAADAOQQYAADgHAIMAABwDgEGAAA4hwADAACcQ4ABAADOIcAAAADnEGAAAIBzCDAAAMA5BBgAAOCc/wNmOOLDRXIhMAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_train_df.groupby('label').count().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset's filter function\n",
    "We can filter the examples using ```filter()``` method. See this link for more details https://huggingface.co/docs/datasets/en/use_dataset. Here is an example of filtering the short tweets (less than 20 characters) from the ```train``` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 506\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "short_tweets = tweet_ds['train'].filter(lambda example: len(example['text']) < 20)\n",
    "print(short_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '️ ️ ️ ️ @ Toys\"R\"Us', 'label': 0}\n",
      "{'text': '️ @ Columbia River', 'label': 12}\n",
      "{'text': 'My weekend: @user 3', 'label': 6}\n",
      "{'text': 'good day today', 'label': 3}\n",
      "{'text': 'My last RT...', 'label': 2}\n",
      "{'text': '@ On Lake Cowichan', 'label': 13}\n",
      "{'text': '@ Macroplaza', 'label': 4}\n",
      "{'text': '@ BART Train', 'label': 6}\n",
      "{'text': '️ 4 a tbh&amp; rate', 'label': 0}\n",
      "{'text': '@user Oh nice!!', 'label': 14}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(short_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset's map function\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries. We will calculate the length of the text (in characters) in each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_text_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'text_length'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'text_length'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'text_length'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tweet_ds = tweet_ds.map(calculate_text_length)\n",
    "print(tweet_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can plot the histogram of the text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(tweet_ds['train']).groupby('text_length')['text_length'].count().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"e1\"></a>\n",
    "\n",
    "### Exercise 1: Questions about the datasets\n",
    "1. (1p) What is the size of the training, test and validation datasets?\n",
    "2. (1p) What are the top 5 most frequent emojis in the validation dataset?\n",
    "3. (1p) Compare the distributions of labels (emojis) between training and validation datasets.\n",
    "4. (1p) How many examples with the \"fire\" emoji are in the training dataset?\n",
    "5. (1p) What is the average length (in characters) of the tweets in the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can add cells here to answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "In this section we will preprocess the dataset by cleaning and tokenizing the entries.\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Write the text cleaning function\n",
    "\n",
    "Include at least the following steps:\n",
    "- (1p) remove comma between numbers, i.e. 15,000 -> 15000\n",
    "- (1p) remove multiple spaces\n",
    "- (1p) space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\")\n",
    "- (3x1p) three more cleaning steps of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean(example):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'clean' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        example['clean'] = ''\n",
    "        return example\n",
    "\n",
    "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
    "    text = str(text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # remove comma between numbers\n",
    "\n",
    "    text = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", text)\n",
    "\n",
    "    # space out the punctuation\n",
    "\n",
    "    text = re.sub(r'([.,!?;:(){}[\\]<>])', r' \\1 ', text)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    \n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "\n",
    "    # three more cleaning steps of your choice\n",
    "\n",
    "    # remove non-standart characters as \"#\" and \"@\"\n",
    "    \n",
    "    text = re.sub(r'([@#])', '', text)\n",
    "\n",
    "    # remove 'a' 'an' articles\n",
    "\n",
    "    text = re.sub(r'\\b[Aa]\\b|\\b[Aa]n\\b', '', text)\n",
    "\n",
    "    # replace \"&\" with \"and\"\n",
    "\n",
    "    text = re.sub(r'\\b&\\b', 'and', text)\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    # Update the example with the cleaned text\n",
    "    example['clean'] = text.strip()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is an example of applying the ```clean()``` function you just wrote to a single entry of the dataset. The function added a 'clean' field to the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet item:\n",
      "Sunday afternoon walking through Venice in the sun with @user ️ ️ ️ @ Abbot Kinney, Venice\n",
      "Cleaned tweet item:\n",
      "Sunday afternoon walking through Venice in the sun with user ️ ️ ️  Abbot Kinney , Venice\n",
      "Original tweet item:\n",
      "Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\n",
      "Cleaned tweet item:\n",
      "Time for some BBQ and whiskey libations . Chomp , belch , chomp ! (  Lucille's Smokehouse Bar-B-Que )\n",
      "Original tweet item:\n",
      "Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…\n",
      "Cleaned tweet item:\n",
      "Love love love all these people ️ ️ ️ friends bff celebrate blessed sundayfunday  San…\n",
      "Original tweet item:\n",
      "️ ️ ️ ️ @ Toys\"R\"Us\n",
      "Cleaned tweet item:\n",
      "️ ️ ️ ️  Toys\"R\"Us\n",
      "Original tweet item:\n",
      "Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme\n",
      "Cleaned tweet item:\n",
      "Man these are the funniest kids ever ! ! That face ! HappyBirthdayBubb  FLIPnOUT Xtreme\n",
      "Original tweet item:\n",
      "#sandiego @ San Diego, California\n",
      "Cleaned tweet item:\n",
      "sandiego  San Diego , California\n",
      "Original tweet item:\n",
      "My little ️ ️ ️ ️ ️ #ObsessedWithMyDog @ Cafe Solstice Capitol Hill\n",
      "Cleaned tweet item:\n",
      "My little ️ ️ ️ ️ ️ ObsessedWithMyDog  Cafe Solstice Capitol Hill\n",
      "Original tweet item:\n",
      "More #tinyepic things #tinyepicwestern, this one is crazy @user I may be one of your…\n",
      "Cleaned tweet item:\n",
      "More tinyepic things tinyepicwestern , this one is crazy user I may be one of your…\n",
      "Original tweet item:\n",
      "Last night ️ @ Omnia Night Club At Caesars Palace\n",
      "Cleaned tweet item:\n",
      "Last night ️  Omnia Night Club At Caesars Palace\n",
      "Original tweet item:\n",
      "friendship at its finest. ....#pixar #toystory #buzz #woody #friends #friendship #bff…\n",
      "Cleaned tweet item:\n",
      "friendship at its finest . . . . . pixar toystory buzz woody friends friendship bff…\n",
      "Original tweet item:\n",
      "I L VE working for a cause! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Cleaned tweet item:\n",
      "I L VE working for  cause ! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Original tweet item:\n",
      "Dem shoes tho! Lol! ...I'm getting some! Get ready copleypriceymca…\n",
      "Cleaned tweet item:\n",
      "Dem shoes tho ! Lol ! . . . I'm getting some ! Get ready copleypriceymca…\n",
      "Original tweet item:\n",
      "I love weekends with you, calebrancourt @ Cannery Row At Monterey Bay, CA\n",
      "Cleaned tweet item:\n",
      "I love weekends with you , calebrancourt  Cannery Row At Monterey Bay , CA\n",
      "Original tweet item:\n",
      "Yesss! August is full of love and birthdays ️ @ Los Angeles,…\n",
      "Cleaned tweet item:\n",
      "Yesss ! August is full of love and birthdays ️  Los Angeles , …\n",
      "Original tweet item:\n",
      "I took a pic with Eminem! *still not over that performance vibe from #parrisproject!…\n",
      "Cleaned tweet item:\n",
      "I took  pic with Eminem ! *still not over that performance vibe from parrisproject ! …\n",
      "Original tweet item:\n",
      "I L VE working for a cause! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Cleaned tweet item:\n",
      "I L VE working for  cause ! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Original tweet item:\n",
      "Sundays are all about the cute babies and dogs! #Ballard #sundaymarket #littelestmodel…\n",
      "Cleaned tweet item:\n",
      "Sundays are all about the cute babies and dogs ! Ballard sundaymarket littelestmodel…\n",
      "Original tweet item:\n",
      "it was too hot .. trying to find shade behind @user at the roadraceengineering booth…\n",
      "Cleaned tweet item:\n",
      "it was too hot . . trying to find shade behind user at the roadraceengineering booth…\n",
      "Original tweet item:\n",
      "Sometimes you just have to be a kid Skyspace, Los Angeles. #CRStylesTravel #FunDay #LALife…\n",
      "Cleaned tweet item:\n",
      "Sometimes you just have to be  kid Skyspace , Los Angeles . CRStylesTravel FunDay LALife…\n",
      "Original tweet item:\n",
      "had an amazing night, with some pretty amazing people @ Pandora Summer\n",
      "Cleaned tweet item:\n",
      "had  amazing night , with some pretty amazing people  Pandora Summer\n",
      "Original tweet item:\n",
      "Thank you @user for an incredible night last night @ Shrine Auditorium &amp; Expo Hall\n",
      "Cleaned tweet item:\n",
      "Thank you user for  incredible night last night  Shrine Auditorium &amp ; Expo Hall\n",
      "Original tweet item:\n",
      "thanks for the amazing night @ Pandora Summer\n",
      "Cleaned tweet item:\n",
      "thanks for the amazing night  Pandora Summer\n",
      "Original tweet item:\n",
      "@user draw the line ~\n",
      "Cleaned tweet item:\n",
      "user draw the line ~\n",
      "Original tweet item:\n",
      "This is why I hate when Taylor visits #Bunny #NotCaturday @ Our House\n",
      "Cleaned tweet item:\n",
      "This is why I hate when Taylor visits Bunny NotCaturday  Our House\n",
      "Original tweet item:\n",
      "thank you to my boys for performing so well last night, I love you all very much @ Pandora…\n",
      "Cleaned tweet item:\n",
      "thank you to my boys for performing so well last night , I love you all very much  Pandora…\n",
      "Original tweet item:\n",
      "2nd theater and 3rd movie of the day a new record #badmoms #suicidesquad #sausageparty (at @user\n",
      "Cleaned tweet item:\n",
      "2nd theater and 3rd movie of the day  new record badmoms suicidesquad sausageparty ( at user\n",
      "Original tweet item:\n",
      "In honor of #happygayunclesday ️ #happygunclesday #guncle #guncles @ Chula Vista, California\n",
      "Cleaned tweet item:\n",
      "In honor of happygayunclesday ️ happygunclesday guncle guncles  Chula Vista , California\n",
      "Original tweet item:\n",
      "I think it's nap time for us @ Los Angeles, California\n",
      "Cleaned tweet item:\n",
      "I think it's nap time for us  Los Angeles , California\n",
      "Original tweet item:\n",
      "I L VE working for a cause! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Cleaned tweet item:\n",
      "I L VE working for  cause ! Yesterday's balloon decor for SNN 11th Annual Back 2 School Health…\n",
      "Original tweet item:\n",
      "Chasing #mclarenp1 in Malibu today.________________________________________ : credit: @user\n",
      "Cleaned tweet item:\n",
      "Chasing mclarenp1 in Malibu today . ________________________________________ : credit : user\n",
      "Original tweet item:\n",
      "Sunday afternoon at home! Drinking good wine and enjoying this incredible view to the sound of…\n",
      "Cleaned tweet item:\n",
      "Sunday afternoon at home ! Drinking good wine and enjoying this incredible view to the sound of…\n",
      "Original tweet item:\n",
      "#Lunch @user best #Ribs in #SanDiego (at @user in San Diego, CA)\n",
      "Cleaned tweet item:\n",
      "Lunch user best Ribs in SanDiego ( at user in San Diego , CA )\n",
      "Original tweet item:\n",
      "Here's to the start of a great adventure. Niners today, Alaska tomorrow. #grams @ Levi's Stadium\n",
      "Cleaned tweet item:\n",
      "Here's to the start of  great adventure . Niners today , Alaska tomorrow . grams  Levi's Stadium\n",
      "Original tweet item:\n",
      "Really horrible seats for a really awful Sunday afternoon! Life is really bad #soundersfc.…\n",
      "Cleaned tweet item:\n",
      "Really horrible seats for  really awful Sunday afternoon ! Life is really bad soundersfc . …\n",
      "Original tweet item:\n",
      "Enjoy your last days of summer! Clearly, mauricethewhale is enjoying his! #ladykiller…\n",
      "Cleaned tweet item:\n",
      "Enjoy your last days of summer ! Clearly , mauricethewhale is enjoying his ! ladykiller…\n",
      "Original tweet item:\n",
      "Surprised everyone by actually surviving a weekend in the wild (however still came close to…\n",
      "Cleaned tweet item:\n",
      "Surprised everyone by actually surviving  weekend in the wild ( however still came close to…\n",
      "Original tweet item:\n",
      "In the zone | : @user #colorsworldwide #RnBOnly @ The Regent\n",
      "Cleaned tweet item:\n",
      "In the zone | : user colorsworldwide RnBOnly  The Regent\n",
      "Original tweet item:\n",
      "“Me &amp; Catherine Sutherland!!!” #PowerMorphicon2016 #PowerRangersZeo #PowerRangersTurbo…\n",
      "Cleaned tweet item:\n",
      "“Me &amp ; Catherine Sutherland ! ! ! ” PowerMorphicon2016 PowerRangersZeo PowerRangersTurbo…\n",
      "Original tweet item:\n",
      "Taped up and dripped out. Know what I'm talking bout #paint #painting #diy #homeimprovement…\n",
      "Cleaned tweet item:\n",
      "Taped up and dripped out . Know what I'm talking bout paint painting diy homeimprovement…\n",
      "Original tweet item:\n",
      "princesscruises @ Port of Seattle, Pier 91\n",
      "Cleaned tweet item:\n",
      "princesscruises  Port of Seattle , Pier 91\n",
      "Original tweet item:\n",
      "Stay classy #SanDiego (@ San Diego International Airport - @user in San Diego, CA)\n",
      "Cleaned tweet item:\n",
      "Stay classy SanDiego (  San Diego International Airport - user in San Diego , CA )\n",
      "Original tweet item:\n",
      "I ️ the people I work with. Enjoying our Sunday watching the first…\n",
      "Cleaned tweet item:\n",
      "I ️ the people I work with . Enjoying our Sunday watching the first…\n",
      "Original tweet item:\n",
      "Another big happy 18th birthday to my partner in crime ️ I love u and all the crazy times we've…\n",
      "Cleaned tweet item:\n",
      "Another big happy 18th birthday to my partner in crime ️ I love u and all the crazy times we've…\n",
      "Original tweet item:\n",
      "Day is DONE! Thank u INSTA-LIKERS! Rollicking day at FRINGE! C U THURSDAY~~~and then some .…\n",
      "Cleaned tweet item:\n",
      "Day is DONE ! Thank u INSTA-LIKERS ! Rollicking day at FRINGE ! C U THURSDAY~~~and then some . …\n",
      "Original tweet item:\n",
      "SnakeShred recording guitars for the new SnakeSkin record #guitar #recording #api #apipegged…\n",
      "Cleaned tweet item:\n",
      "SnakeShred recording guitars for the new SnakeSkin record guitar recording api apipegged…\n",
      "Original tweet item:\n",
      "My sisters, my best friends #hamessisters #andlewisandclark…\n",
      "Cleaned tweet item:\n",
      "My sisters , my best friends hamessisters andlewisandclark…\n",
      "Original tweet item:\n",
      "my absolute favorite place. @ La Jolla Cove Seal Beach\n",
      "Cleaned tweet item:\n",
      "my absolute favorite place .  La Jolla Cove Seal Beach\n",
      "Original tweet item:\n",
      "To live is an awfully big adventure #Seattle #spaceneedle @ Space Needle\n",
      "Cleaned tweet item:\n",
      "To live is  awfully big adventure Seattle spaceneedle  Space Needle\n",
      "Original tweet item:\n",
      "Finally met Snow White ! She was my favorite when i was little :) #SnowWhite #Disneyland @user\n",
      "Cleaned tweet item:\n",
      "Finally met Snow White ! She was my favorite when i was little : ) SnowWhite Disneyland user\n",
      "Original tweet item:\n",
      "@user loved your coverage at TI as always, now need to get you on some of those Sounders matches...\n",
      "Cleaned tweet item:\n",
      "user loved your coverage at TI as always , now need to get you on some of those Sounders matches . . .\n",
      "Original tweet item:\n",
      "Saturdayzzz w/ Lionel Richie @ Harvey's Outdoor Ampitheater\n",
      "Cleaned tweet item:\n",
      "Saturdayzzz w/ Lionel Richie  Harvey's Outdoor Ampitheater\n",
      "Original tweet item:\n",
      "All the beautiful ladies that helped make last night #APositiveExperience @ Upscale Salon\n",
      "Cleaned tweet item:\n",
      "All the beautiful ladies that helped make last night APositiveExperience  Upscale Salon\n",
      "Original tweet item:\n",
      "Sunday funday indeed #sundayfunday #igers #igdaily #instalike #enjoylife #asian #LA…\n",
      "Cleaned tweet item:\n",
      "Sunday funday indeed sundayfunday igers igdaily instalike enjoylife asian LA…\n",
      "Original tweet item:\n",
      "Lights, Camera, Action @ Hollywood Walk of Fame\n",
      "Cleaned tweet item:\n",
      "Lights , Camera , Action  Hollywood Walk of Fame\n",
      "Original tweet item:\n",
      "Found your Unicorn @user it's a pillow/stuffed toy and every pride weekend it…\n",
      "Cleaned tweet item:\n",
      "Found your Unicorn user it's  pillow/stuffed toy and every pride weekend it…\n",
      "Original tweet item:\n",
      "Harry Potter ️ @ TCL Chinese Theatres\n",
      "Cleaned tweet item:\n",
      "Harry Potter ️  TCL Chinese Theatres\n",
      "Original tweet item:\n",
      "My key was still here...thank the Lawd #littlehoneyvee #doodles #doodleoftheday #illustration…\n",
      "Cleaned tweet item:\n",
      "My key was still here . . . thank the Lawd littlehoneyvee doodles doodleoftheday illustration…\n",
      "Original tweet item:\n",
      "️ ️ ️ @ Los Angeles, California\n",
      "Cleaned tweet item:\n",
      "️ ️ ️  Los Angeles , California\n",
      "Original tweet item:\n",
      "Absolutely in love with this ethereal glow on holytolidoitseric. Who would rock this look?…\n",
      "Cleaned tweet item:\n",
      "Absolutely in love with this ethereal glow on holytolidoitseric . Who would rock this look ? …\n",
      "Original tweet item:\n",
      "Getting up that ice wall one ax at a time : pechoi11 @ Mount Shasta, California\n",
      "Cleaned tweet item:\n",
      "Getting up that ice wall one ax at  time : pechoi11  Mount Shasta , California\n",
      "Original tweet item:\n",
      "Sephora is my favorite place @ SEPHORA\n",
      "Cleaned tweet item:\n",
      "Sephora is my favorite place  SEPHORA\n",
      "Original tweet item:\n",
      "Tutorial time w/ @user @user @ Stern Grove Festival\n",
      "Cleaned tweet item:\n",
      "Tutorial time w/ user user  Stern Grove Festival\n",
      "Original tweet item:\n",
      "I love my people! @ Paso Robles, California\n",
      "Cleaned tweet item:\n",
      "I love my people !  Paso Robles , California\n",
      "Original tweet item:\n",
      "Rad Fashion at the midway SF on a Sunday ️ ️ ️ @ The Midway SF\n",
      "Cleaned tweet item:\n",
      "Rad Fashion at the midway SF on  Sunday ️ ️ ️  The Midway SF\n",
      "Original tweet item:\n",
      "Just like the movie... Radiator Springs ️ #cars #inlove…\n",
      "Cleaned tweet item:\n",
      "Just like the movie . . . Radiator Springs ️ cars inlove…\n",
      "Original tweet item:\n",
      "Cheers to Bruce on his birthday. ️#westhollywood…\n",
      "Cleaned tweet item:\n",
      "Cheers to Bruce on his birthday . ️westhollywood…\n",
      "Original tweet item:\n",
      "My bears! #cityfest ️ ️ ️ @ Babycakes San Diego\n",
      "Cleaned tweet item:\n",
      "My bears ! cityfest ️ ️ ️  Babycakes San Diego\n",
      "Original tweet item:\n",
      "gonna miss you guys so much. thank you for an amazing experience ️#ballyhoocarrie…\n",
      "Cleaned tweet item:\n",
      "gonna miss you guys so much . thank you for  amazing experience ️ballyhoocarrie…\n",
      "Original tweet item:\n",
      "MATTRESS HELPER - new again SAGGING #BED #FIRMER #fix #guaranteed #sleepless #needsleep…\n",
      "Cleaned tweet item:\n",
      "MATTRESS HELPER - new again SAGGING BED FIRMER fix guaranteed sleepless needsleep…\n",
      "Original tweet item:\n",
      "Just that sound alone makes the world stop ️ @ Sausalito, California\n",
      "Cleaned tweet item:\n",
      "Just that sound alone makes the world stop ️  Sausalito , California\n",
      "Original tweet item:\n",
      "#ExclusiveBaseballPicks are KILLING the #Bookie like always. #WeeklyRecap 8/8-8/14. 96-3…\n",
      "Cleaned tweet item:\n",
      "ExclusiveBaseballPicks are KILLING the Bookie like always . WeeklyRecap 8/8-8/14 . 96-3…\n",
      "Original tweet item:\n",
      "My lady love beautiful friend #doubleLtrouble #leilavie #beachour @ Stinson Beach, California\n",
      "Cleaned tweet item:\n",
      "My lady love beautiful friend doubleLtrouble leilavie beachour  Stinson Beach , California\n",
      "Original tweet item:\n",
      "My date for tonight @ Hotel Las Rocas\n",
      "Cleaned tweet item:\n",
      "My date for tonight  Hotel Las Rocas\n",
      "Original tweet item:\n",
      "This weekend love was truly in the air but out of all the celeb weddings I have to say…\n",
      "Cleaned tweet item:\n",
      "This weekend love was truly in the air but out of all the celeb weddings I have to say…\n",
      "Original tweet item:\n",
      "@user hi from Tijuana B.C. See you soon\n",
      "Cleaned tweet item:\n",
      "user hi from Tijuana B . C . See you soon\n",
      "Original tweet item:\n",
      "️ #gramma @ Living Way Christian Fellowship\n",
      "Cleaned tweet item:\n",
      "️ gramma  Living Way Christian Fellowship\n",
      "Original tweet item:\n",
      "Acquired this stunner today at my new favorite spot Sharks Tooth #crassula.…\n",
      "Cleaned tweet item:\n",
      "Acquired this stunner today at my new favorite spot Sharks Tooth crassula . …\n",
      "Original tweet item:\n",
      "Birthday cheers with my favorite people!!#26again #justafewofmygirls #justafewofmygirls…\n",
      "Cleaned tweet item:\n",
      "Birthday cheers with my favorite people ! ! 26again justafewofmygirls justafewofmygirls…\n",
      "Original tweet item:\n",
      "today was my first day of work at Victoria Secret SO happy and SO…\n",
      "Cleaned tweet item:\n",
      "today was my first day of work at Victoria Secret SO happy and SO…\n",
      "Original tweet item:\n",
      "Taking bae to all my favorite #LA spots. #gettycenter @ Getty Center\n",
      "Cleaned tweet item:\n",
      "Taking bae to all my favorite LA spots . gettycenter  Getty Center\n",
      "Original tweet item:\n",
      "So much fun this weekend celebrating the bride to be. #MalliesLastRally @ Las Vegas, Nevada\n",
      "Cleaned tweet item:\n",
      "So much fun this weekend celebrating the bride to be . MalliesLastRally  Las Vegas , Nevada\n",
      "Original tweet item:\n",
      "Love this picture of me and baby Eve at Disneyland earlier today …\n",
      "Cleaned tweet item:\n",
      "Love this picture of me and baby Eve at Disneyland earlier today …\n",
      "Original tweet item:\n",
      "The one photo I've taken with my Mother that I like for once. -08/13/16, Sat. @ El Dorado…\n",
      "Cleaned tweet item:\n",
      "The one photo I've taken with my Mother that I like for once . -08/13/16 , Sat .  El Dorado…\n",
      "Original tweet item:\n",
      "Love my bed @ Downey, California\n",
      "Cleaned tweet item:\n",
      "Love my bed  Downey , California\n",
      "Original tweet item:\n",
      "So cute!! #KnottsBerryFarm #Charlie #Snoopy #Woodstock @ Knott's Berry Farm\n",
      "Cleaned tweet item:\n",
      "So cute ! ! KnottsBerryFarm Charlie Snoopy Woodstock  Knott's Berry Farm\n",
      "Original tweet item:\n",
      "I ️ Faure Requiem @user oboyddd @ Walt Disney Concert Hall\n",
      "Cleaned tweet item:\n",
      "I ️ Faure Requiem user oboyddd  Walt Disney Concert Hall\n",
      "Original tweet item:\n",
      "️ @ Columbia River\n",
      "Cleaned tweet item:\n",
      "️  Columbia River\n",
      "Original tweet item:\n",
      "No I don't have any carrots!! #seattlepoloclub #horse #horses @ Seattle Polo &amp; Equestrian Club\n",
      "Cleaned tweet item:\n",
      "No I don't have any carrots ! ! seattlepoloclub horse horses  Seattle Polo &amp ; Equestrian Club\n",
      "Original tweet item:\n",
      "My favourite place in California #YosemiteNationalPark #California @ Yosemite Wilderness\n",
      "Cleaned tweet item:\n",
      "My favourite place in California YosemiteNationalPark California  Yosemite Wilderness\n",
      "Original tweet item:\n",
      "#sundayfunday #live at allure... @user @user @user\n",
      "Cleaned tweet item:\n",
      "sundayfunday live at allure . . . user user user\n",
      "Original tweet item:\n",
      "My babies ️ ️ ️ #cat #dog #catsofinstagram #dogsofinstagram #lovemyfurbabies @ Port Hueneme,…\n",
      "Cleaned tweet item:\n",
      "My babies ️ ️ ️ cat dog catsofinstagram dogsofinstagram lovemyfurbabies  Port Hueneme , …\n",
      "Original tweet item:\n",
      "Photo taken Thursday at SDCC2016 by MannyLLanura Photography. #widowmaker #overwatch…\n",
      "Cleaned tweet item:\n",
      "Photo taken Thursday at SDCC2016 by MannyLLanura Photography . widowmaker overwatch…\n",
      "Original tweet item:\n",
      "Hello there, my king Jareth #bowie #goblinking #labyrinth @ Los Angeles, California\n",
      "Cleaned tweet item:\n",
      "Hello there , my king Jareth bowie goblinking labyrinth  Los Angeles , California\n",
      "Original tweet item:\n",
      "These Teens @ ARIA Resort &amp; Casino\n",
      "Cleaned tweet item:\n",
      "These Teens  ARIA Resort &amp ; Casino\n",
      "Original tweet item:\n",
      "Sunday views #hike #waterfall #waterfallhike #latourellfalls #hikeoregon #hikepnw #whyihike…\n",
      "Cleaned tweet item:\n",
      "Sunday views hike waterfall waterfallhike latourellfalls hikeoregon hikepnw whyihike…\n",
      "Original tweet item:\n",
      "\"Beauty begins the moment you decide to be yourself\" -Coco Chanel love this!Skin is the largest…\n",
      "Cleaned tweet item:\n",
      "\"Beauty begins the moment you decide to be yourself\" -Coco Chanel love this ! Skin is the largest…\n",
      "Original tweet item:\n",
      "Caught by the lens of Yc Wong. photo by 8888tiger8888 suit by virginblak #ljifff…\n",
      "Cleaned tweet item:\n",
      "Caught by the lens of Yc Wong . photo by 8888tiger8888 suit by virginblak ljifff…\n",
      "Original tweet item:\n",
      "Weldone champ #babrain #alikhamis #rio #rio2016 #olympics @ Los…\n",
      "Cleaned tweet item:\n",
      "Weldone champ babrain alikhamis rio rio2016 olympics  Los…\n",
      "Original tweet item:\n",
      "Took a lil trip to Tahoe #iputhehoeintahoe @ Incline Village At…\n",
      "Cleaned tweet item:\n",
      "Took  lil trip to Tahoe iputhehoeintahoe  Incline Village At…\n",
      "Original tweet item:\n",
      "About to hang out with a bunch of #Armenians and #Russians...trying to fit in Sporting my…\n",
      "Cleaned tweet item:\n",
      "About to hang out with  bunch of Armenians and Russians . . . trying to fit in Sporting my…\n",
      "Original tweet item:\n",
      "️ @ Yosemite Wilderness\n",
      "Cleaned tweet item:\n",
      "️  Yosemite Wilderness\n",
      "Original tweet item:\n",
      "Yum! @ Rosa's Cantina\n",
      "Cleaned tweet item:\n",
      "Yum !  Rosa's Cantina\n",
      "Original tweet item:\n",
      "@ Friends of Owen's Playground\n",
      "Cleaned tweet item:\n",
      "Friends of Owen's Playground\n",
      "Original tweet item:\n",
      "@user catchin a lil sun ️#goldenbaby_franki #myminime #summertime…\n",
      "Cleaned tweet item:\n",
      "user catchin  lil sun ️goldenbaby_franki myminime summertime…\n",
      "Original tweet item:\n",
      "My Sunday morning 1200 lunches with hygiene items ️ thank you so much to everyone who donated…\n",
      "Cleaned tweet item:\n",
      "My Sunday morning 1200 lunches with hygiene items ️ thank you so much to everyone who donated…\n",
      "Original tweet item:\n",
      "Our hotel is so hipster, this is what the lobby looks like it's awesome tho @ Portland, Oregon\n",
      "Cleaned tweet item:\n",
      "Our hotel is so hipster , this is what the lobby looks like it's awesome tho  Portland , Oregon\n",
      "Original tweet item:\n",
      "Dayum it's hot out here #vegasbaby #turnuptheheat #vegasshows #projectshows #libertyfairs…\n",
      "Cleaned tweet item:\n",
      "Dayum it's hot out here vegasbaby turnuptheheat vegasshows projectshows libertyfairs…\n",
      "Original tweet item:\n",
      "Sunday vibes are the best ️ @ Victory Missionary Baptist Church\n",
      "Cleaned tweet item:\n",
      "Sunday vibes are the best ️  Victory Missionary Baptist Church\n",
      "Original tweet item:\n",
      "Photoshooting with the babe @user #sundayfunday @ Downtown Container Park\n",
      "Cleaned tweet item:\n",
      "Photoshooting with the babe user sundayfunday  Downtown Container Park\n",
      "Original tweet item:\n",
      "#daddyslilmonster @ Hot Topic Stonewood\n",
      "Cleaned tweet item:\n",
      "daddyslilmonster  Hot Topic Stonewood\n"
     ]
    }
   ],
   "source": [
    "for i in range(110):\n",
    "    print('Original tweet item:')\n",
    "    print(tweet_ds['train'][i]['text'])\n",
    "    print('Cleaned tweet item:')\n",
    "    print(clean(tweet_ds['train'][i])['clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. You can see that the ```clean``` column has been added to each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below, we will apply your function to all entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_ds = tweet_ds.map(clean)\n",
    "print(tweet_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Build vocabulary\n",
    "\n",
    "In the previous section, we implemented the cleaning of the dataset. Now, we will tokenize the text splitting it by spaces. We will build a vocabulary based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law).\n",
    "\n",
    "The function below builds a vocabulary from the dataset. It counts the occurrences of the words in the dataset using the Counter class. Check the documentation here [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3: Build the vocabulary\n",
    "(5p) Fill in the function below to build the vocabulary from the dataset. The function should return a `Counter` object with the words and their frequencies. The variable named `vocab` is already initialized as an empty `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_counter = build_vocab_counter(tweet_ds['train'])\n",
    "print('Size of the vocabulary:', len(vocab_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because we created a counter, we can easily check the most and least common words in the vocabulary. Do the most common words make sense? How about the least common ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Most common:')\n",
    "print(vocab_counter.most_common(10))\n",
    "print('Least common:')\n",
    "print(vocab_counter.most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.loglog([val for word, val in vocab_counter.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plot shows that the distribution of the words in the vocabulary follows the Zipf's law. The most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n",
    "\n",
    "We can also filter the vocabulary by the frequency of the words. We will only consider the most frequent words and mark the rest as the `<unk>` token. Here we set the maximum vocabulary size to 10,000. But in the later steps, you will experiment with different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 4: Frequency of pairs of words (bigrams)\n",
    "Calculate the frequency of (neighbouring) pairs of words in the training dataset.\n",
    "- (5p) List the most and least common pairs. Do the most common pairs make sense?\n",
    "- (2p) How many pairs occur only once in the dataset?\n",
    "- (5p) Plot the distribution of the pair frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Tokenize the dataset\n",
    "The function below tokenizes the cleaned text (```example['clean']```) by splitting it on spaces. It replaces the words that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 5: Tokenize the dataset\n",
    "\n",
    "(5p) Fill in the function below to tokenize the dataset. The function will be applied to the dataset through the `map()` method, so it returns the updated example. Your task is to split the text by spaces and replace the words that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example, vocab, unknown_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a vocabulary as a list of words\n",
    "        unknown_token: a token to replace the words that are not in the vocabulary\n",
    "    Returns: update example containing 'tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    tokens = None # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    example['tokens'] = tokens\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_ds = tweet_ds.map(tokenize, fn_kwargs={'vocab': vocab})\n",
    "print(tweet_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us examine several entries from the dataset. We can see that the `tokens` column has been added to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['train'][i]['text'])\n",
    "    print('Tokenized tweet:')\n",
    "    print(tweet_ds['train'][i]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Make sure that the tokenization works as you intended. If not, revisit the cleaning and tokenization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 6: Questions about the tokenization\n",
    "1. (3p) How many unknown tokens are in the validation dataset after tokenization?\n",
    "2. (3p) What is the distribution of the number of tokens in the training dataset?\n",
    "3. (4p) How the number of tokens corresponds to the number of characters in our dataset?\n",
    "4. (4p) How the size of the vocabulary (```max_vocab_size```) affects the number of unknown tokens?\n",
    "5. (4p) How does the size of the vocabulary affect the number of tokens in the dataset?\n",
    "6. (4p) Think about the advantages and disadvantages of the tokenization method we used. What are the cases when it will not work well?\n",
    "\n",
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Byte Pair Encoding\n",
    "\n",
    "In this section, you will build the Byte Pair Encoding (BPE) tokenizer. BPE is an algorithm that replaces the most frequent pair of tokens (initially characters) with a new token. The algorithm is configured by the number of merges that are performed. You can find the paper here [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Finding the initial set of characters\n",
    "BPE algorithm starts with the set of characters that occur in the dataset. We will build a character counter from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 7: Counting the characters\n",
    "\n",
    "(5p) In this exercise, we build a counter with the frequencies of all characters in the dataset. Iterate over the dataset and count the characters in the `clean` column. The function returns a `Counter` object with the characters and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_character_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a character counter from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a character counter\n",
    "\n",
    "    \"\"\"\n",
    "    char_counter = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return char_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next cell applies the function to the training dataset and prints the size of the vocabulary and the most common characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char_counter = build_character_counter(tweet_ds['train'])\n",
    "print(len(char_counter))\n",
    "print(char_counter.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will filter the characters that occur less than 10 times in the dataset. We will also replace the space character with the `__` token. This is necessary because we want to preserve the spaces between the words in the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpe_init_vocab = sorted([char for char, _ in char_counter.most_common() if char_counter[char] >= 10])\n",
    "bpe_init_vocab[bpe_init_vocab.index(' ')] = '__'\n",
    "print(bpe_init_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Training the BPE tokenizer\n",
    "\n",
    "In this section, we will implement the BPE algorithm. We will start by initializing the BPE corpus. The corpus is a list of words from the dataset with their frequency. This makes it easier to find the most frequent pairs of neighbouring tokens (or characters in the beginning). Each word is split into characters and the space (the ```__``` token) is added at the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_bpe_corpus(dataset):\n",
    "    \"\"\"\n",
    "    Initializes the BPE corpus\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a BPE corpus\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = Counter()\n",
    "    for example in dataset:\n",
    "        words = example['clean'].split()\n",
    "        words = [' '.join(list(word)) + ' __' for word in words]\n",
    "        corpus.update(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpe_corpus = init_bpe_corpus(tweet_ds['train'])\n",
    "print(len(bpe_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the most common words in the corpus along with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpe_corpus.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our BPE implementation will consist of the following steps:\n",
    "1. Calculate the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "2. Find the most frequent pair.\n",
    "3. Merge the most frequent pair.\n",
    "4. Repeat until the specified number of merges is reached.\n",
    "\n",
    "The following function calculates the frequency statistics of adjacent symbol pairs in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 8: Calculate the frequency statistics of adjacent symbol pairs\n",
    "(5p) Fill in the function below to calculate the frequency statistics of adjacent symbol pairs in the corpus. The function returns a Counter object with the counts of adjacent token pairs. The pairs are represented as tuples of two tokens (e.g., `('cali', 'for')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bpe_corpus_stats(corpus):\n",
    "    \"\"\"\n",
    "    Calculates the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "    Args:\n",
    "        corpus: a BPE corpus as a Counter object with words split by space into tokens (initially characters)\n",
    "\n",
    "    Returns: a Counter object with the frequency statistics of adjacent symbol pairs\n",
    "    \"\"\"\n",
    "    stats = Counter()\n",
    "\n",
    "    for word, freq in corpus.items():\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the most common pairs of characters in the initial corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats = calculate_bpe_corpus_stats(bpe_corpus)\n",
    "print(stats.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will implement the function that merges the most frequent pair of symbols in the corpus. The function takes the corpus and the most frequent pair of symbols as input and returns the updated corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_corpus(corpus, pair):\n",
    "    \"\"\"\n",
    "    Merges the most frequent pair of symbols in the corpus.\n",
    "    Args:\n",
    "        corpus (dict): Keys are words as space-separated symbols (e.g., \"l o w\"),\n",
    "                       and values are the frequency counts.\n",
    "        pair (tuple): A pair of symbols to merge.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated corpus after merging the pair of symbols.\n",
    "    \"\"\"\n",
    "    new_corpus = Counter()\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    for word, freq in corpus.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_corpus[new_word] = freq\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last step is to implement the BPE algorithm. The function takes the initial vocabulary, the corpus, and the number of merges as input. It returns the updated vocabulary, corpus, and the list of merges.\n",
    "Returning the list of merges is useful for the tokenization process - it makes it faster to tokenize the text. It contains the tuples of the two tokens that were merged. For example, ('to', 'day__') will merge the tokens 'to' and 'day__' into the 'today__' token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 9: BPE algorithm\n",
    "\n",
    "(10p) Implement the BPE algorithm in the following function. The function should return the updated vocabulary, corpus, and the list of merges. The function should perform the specified number of merges. The vocabulary is a list of tokens, the corpus is a Counter object with the words split by space into tokens, and the merges is a list of tuples with the merged tokens.\n",
    "\n",
    "You should use the functions you implemented earlier in this section (```calculate_bpe_corpus_stats()```, ```merge_corpus()```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bpe(vocab, corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Applies the BPE algorithm to the corpus. Merges the most frequent adjacent symbol pairs. The function performs the specified number of merges.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): A list of tokens representing the BPE vocabulary.\n",
    "        corpus (Counter): A Counter object with words split by space into tokens.\n",
    "        num_merges (int): The number of merges to perform.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated vocabulary.\n",
    "        Counter: Updated corpus.\n",
    "        list: List of merges.\n",
    "    \"\"\"\n",
    "    vocab = vocab.copy()\n",
    "    corpus = corpus.copy()\n",
    "    merges = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_merges)):\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return vocab, corpus, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following cell applies the BPE algorithm to the initial vocabulary and corpus. We will perform 100 merges at first, but you will experiment with different numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bpe_vocab, updated_bpe_corpus, bpe_merges = bpe(bpe_init_vocab, bpe_corpus, num_merges=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the size of the BPE vocabulary and the most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(bpe_vocab))\n",
    "print(bpe_vocab[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also check the most common merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(bpe_merges[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Tokenizing the text using BPE\n",
    "\n",
    "With the tokenizer trained we can now tokenize the text using the BPE vocabulary. We will first build a function that tokenizes any text using our BPE tokenizer (vocabulary and merges). Next we will apply it to our dataset.\n",
    "\n",
    "The following function tokenizes the text using the BPE vocabulary. It replaces the most frequent pairs of tokens with the new token. The function also replaces the tokens that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bpe_tokenization(text, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using BPE vocabulary, preserving spaces as '__'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        vocab (set): A set containing the BPE vocabulary tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens representing the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    words = re.split(r'\\s', text)\n",
    "    words = [' ' + ' '.join(list(word)) + (' __ ' if i < len(words) - 1 else ' ') for i, word in enumerate(words)]\n",
    "\n",
    "    bpe_tokens = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for merge in merges:\n",
    "            word = word.replace(' ' + ' '.join(merge) + ' ', ' ' + ''.join(merge) + ' ')\n",
    "        bpe_tokens.extend(word.split())\n",
    "\n",
    "    for i, token in enumerate(bpe_tokens):\n",
    "        if token not in vocab:\n",
    "            bpe_tokens[i] = unk_token\n",
    "    return bpe_tokens\n",
    "\n",
    "\n",
    "# A test example with a special character. Is the character tokenized correctly as <unk> token?\n",
    "print(apply_bpe_tokenization(tweet_ds['train'][0]['clean'] + ' 🇺', bpe_vocab, bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function below will apply our BPE tokenizer to the dataset. It will add a new column `bpe_tokens` to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_bpe(example, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset using BPE\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a BPE vocabulary\n",
    "\n",
    "    Returns: update example containing 'bpe_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    bpe_tokens = apply_bpe_tokenization(text, vocab, merges, unk_token)\n",
    "    example['bpe_tokens'] = bpe_tokens\n",
    "    return example\n",
    "\n",
    "tweet_ds = tweet_ds.map(tokenize_bpe, fn_kwargs={'vocab': bpe_vocab, 'merges': bpe_merges})\n",
    "print(tweet_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will inspect the both tokenizations of several examples from the ```validation``` subset. Try to find the ```<unk>``` tokens in the printed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['validation'][i]['text'])\n",
    "    print('Word tokenization:')\n",
    "    print(tweet_ds['validation'][i]['tokens'])\n",
    "    print('BPE tokenization:')\n",
    "    print(tweet_ds['validation'][i]['bpe_tokens'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 10: Comparing tokenizers\n",
    "\n",
    "Train the BPE tokenizer with different number of merges. Compare the tokenization results with the word tokenization.\n",
    "1. (5p) What are the differences?\n",
    "2. (5p) Compare the number of tokens created by your tokenizers.\n",
    "3. (5p) Calculate the number of `<unk>` tokens in the validation dataset for each tokenizer.\n",
    "4. (5p) Compare the average length in tokens between different tokenizers.\n",
    "5. (5p) What are the advantages and disadvantages of the BPE tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
